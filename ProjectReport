FINAL TERM PROJECT
Data Mining 
                                                        
 
Supervised Data Mining – Classification
Category 3(Decision Trees)
Category 5(Naïve Bayes)

	Software Tool - Category 10(Python)	
                                                                       https://scikit-learn.org/
 

Name: Jhona Davied D souza
UCID: jd655

Table of Contents

Serial No	Topic	Page No
1.	Abstract	3
2.	Naïve Bayes Classification Algorithm	
	2.1. Source Code	               4-5
	2.2 Steps of Execution	6-13
	2.3 Screenshot showing running of Naïve Bayes	14
3.	Decision Trees Classification Algorithm	
	3.1. Source Code	15-16
	3.2. Steps of Execution	17 – 24
	3.3. Screenshot showing running of Decision Trees Classification 	25
4. 	Summary	26


ABSTRACT


Goal
To evaluate the performance of classification algorithms namely Naïve Bayes and Decision Tree Classifier in predicting the value of independent variables
About the Dataset
The dataset consists of four fields namely User Id, Age, Estimated Salary and Purchased. The data is that of users who bought an SUV. The machine has to predict for given age and estimated salary whether a user will buy the SUV or not. 
We first have to train the model on a training set which is 75 per cent of the whole data and then predict results on a test set of remaining 25 per cent data
Software Used
1.	Anaconda Python(https://www.anaconda.com/distribution/)
2.	IDE: Spyder from Anaconda
3.	Dataset: https://www.kaggle.com/rakeshrau/social-network-ads
Libraries used
1.	https://scikit-learn.org/













NAÏVE BAYES CLASSIFICATION ALGORITHM

Source Code
# -*- coding: utf-8 -*-
"""
Created on Sat Nov 23 10:58:41 2019

@author: jhona
"""

# Naive Bayes

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('Social_Network_Ads.csv')
X = dataset.iloc[:, [2, 3]].values
y = dataset.iloc[:, 4].values

# Splitting the dataset into the Training set and Test set
from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Fitting Naive Bayes to the Training set
from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(X_train, y_train)

# Predicting the Test set results
y_pred = classifier.predict(X_test)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

# Visualising the Training set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_train, y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Naive Bayes (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

# Visualising the Test set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_test, y_test
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Naive Bayes (Test set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()












Steps of Execution
1.	Set the working directory where your program file and dataset will be stored

2.	Import the libraries 
a)	Numpy – math libraries
b)	Matplotlib- library used to plot charts
c)	Pandas – to import and manage datasets
 

3.	Import the dataset

Use pandas to import the dataset

a)	Pd.read_csv(‘Social_Network_Ads.csv’)

 
 


b)	Distinguish matrix of features (independent variables) and dependent variables

a)	
             
– The second and third column Age and Estimated salary will be the independent variables


 



b)	
 

The fourth column Purchased will be the dependent variable

 

 



4.	Splitting the dataset into training set and test set

(Using the cross_validation library from sckit-learn)

 

a)	Create X_train, X_test, y_train, y_test

b)	Use train_test_split(X,y,test_size=0.25, random_state= 0) – 25 percent of data goes to test set remaining training set , random_state=0 gives same result everytime


 

5.	Features Scaling

Due to difference in scale of age and estimated salary the Euclidean distance won’t be measured 
accurately and hence we need to use feature scaling 

We use the preprocessing library from sckit-learn.org and import Standard Scaler
to fit_and_transform training set and test set


 

//for X_test we don’t have to use fit since it is already fitted to the training set

 


6.	Fitting Naïve Bayes classifier to the training set

Use the GaussianNB from Naïve Bayes library from sckit-learn.org
Fit the Naïve Bayes classifier to X_train and Y_train

 

7.	Predicting the test set results

Gets the vector of predictions y_pred

 

Compare y_pred and y_test we see first 6 predictions are same 

(For Naïve Bayes)
 

We observe first 8 predictions are correct but 9th prediction is 1( customer purchased the product) contrary to y_test


8.	Making the confusion matrix

 


The confusion matrix evaluates the number of correct and incorrect predictions

 

We have 65+25=90 correct predictions and 7+3=10 incorrect predictions which is quite good

9.	Visualization of training set results




 


The red points represent the users who didn’t buy the SUV, whereas the green points represent the users who bought the SUV. 
The red region represents the users who didn’t buy the SUV as predicted by the classifier and the green region represents the users who bought the SUV.
As you can see from the graph there are some green points inside the red region, these are the points that were predicted by the classifier as users who didn’t buy the SUV but actually did buy the SUV
Whereas there are some red points inside the green region which represents the users that were predicted to buy the SUV but actually didn’t buy it.



10.	Visualizing the test set results



 


As you can see the test set results are pretty impressive, most of the red points are in the red region and the green points in the green region.

If you count the number of green points in the red region plus the number of red points in the green region you shall get a total of 10 which was the result given by the confusion matrix













Screenshot showing the running of Naïve Bayes

 


























Decision Trees Classification

Source code
# -*- coding: utf-8 -*-
"""
Created on Sat Nov 23 11:33:07 2019

@author: jhona
"""

# Decision Tree Classification

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('Social_Network_Ads.csv')
X = dataset.iloc[:, [2, 3]].values
y = dataset.iloc[:, 4].values

# Splitting the dataset into the Training set and Test set
from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Fitting Decision Tree Classification to the Training set
from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifier.fit(X_train, y_train)

# Predicting the Test set results
y_pred = classifier.predict(X_test)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

# Visualising the Training set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_train, y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Decision Tree Classification (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

# Visualising the Test set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_test, y_test
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Decision Tree Classification (Test set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()










Steps of Execution
1.	Set the working directory where your program file and dataset will be stored

2.	Import the libraries 
d)	Numpy – math libraries
e)	Matplotlib- library used to plot charts
f)	Pandas – to import and manage datasets
 

3.	Import the dataset

Use pandas to import the dataset

c)	Pd.read_csv(‘Social_Network_Ads.csv’)

 
 


d)	Distinguish matrix of features (independent variables) and dependent variables

c)	
             
– The second and third column Age and Estimated salary will be the independent variables


 



d)	
 

The fourth column Purchased will be the dependent variable

 

 



4.	Splitting the dataset into training set and test set

(Using the cross_validation library from sckit-learn)

 

c)	Create X_train, X_test, y_train, y_test

d)	Use train_test_split(X,y,test_size=0.25, random_state= 0) – 25 percent of data goes to test set remaining training set , random_state=0 gives same result everytime


 

5.	Features Scaling

Due to difference in scale of age and estimated salary the Euclidean distance won’t be measured 
accurately and hence we need to use feature scaling 

We use the preprocessing library from sckit-learn.org and import Standard Scaler
Fit_and_transform training set and test set


 

//for X_test we don’t have to use fit since it is already fitted to the training set

 



6.	Fitting Decision Tree classifier to the training set

Import Decision tree classifier from library tree of sckit-learn.org
We use criterion = ‘entropy’ which makes the child nodes at the root more homogenous and gives better quality results than Naïve Bayes
Fit the classifier to the training set X_train and y_train

 







7.	Predicting the test set results

Gets the vector of predictions y_pred

 

Compare y_pred and y_test we see first 6 predictions are same 

(For Naïve Bayes)
 

We observe till 12 all predictions are correct but at 13 there is one wrong prediction and so on there are more wrong predictions




8.	Making the confusion matrix
The confusion matrix evaluates the number of correct and incorrect predictions
 
We have 62+29=91 correct predictions and 6+3=9 incorrect predictions which is quite good compared to Naïve bayes which is 90 correct predictions and 10 incorrect predictions

9.	Visualization of training set results

 
This is more accurate than Naïve Bayes because it is trying to catch every user at the right place
The prediction boundary is only horizontal and vertical lines, it is making splits based on the conditions of independent variables i.e age and estimated salary
10.	Visualizing the test set results

 
As we can see there are some green points in the red region and some red points in the green region
If we count the number of incorrect predictions we will get it to be 9
Also we see some red rectangles that don’t seem to have any points in them.
But compared to the Naïve Bayes this seems to be comparatively better.
















Screenshot showing the running of Decision Tree Classifier

Running of Program

 













Summary

1.	We have observed two classification algorithms Naïve Bayes and Decision Trees Classification
2.	We observe that Naïve Bayes has a curve which divides the points into two regions one where the user is supposed to buy the SUV and another region where the user doesn’t buy the SUV
3.	Though Naïve Bayes is better compared to linear regression where the graph is a straight line it has comparatively lesser accuracy as compared to Decision Trees
4.	In Decision trees the graph gets broken into splits thus covering the points according to the predictions more accurately


